---
title: "统计学习导论——介绍"
author: "李峰"
institute: "统计学院<br/>江西财经大学"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [xaringan-themer.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r xaringan-themer, include = FALSE, warning = FALSE}
library(xaringanthemer)
style_solarized_light()

```


```{r setup, include=FALSE}
library("ElemStatLearn")
library("mlr")
library("ggplot2")
library("ISLR")
library("ggplot2")
library("MASS")
require("plotly")
library("plotly")
set.seed(3)
```

### 统计学习导论的初步介绍


1. 作为R的IDE的RStudio

2. R语言基础回顾

3. 统计学习导论初步


---


class: center, middle

## 1. 作为R的IDE的RStudio


---

### 新建R文件

对于Rstudio，平时最常用的功能就是用它来进行编程或写作，对应的，可以建立两类文件来达成我们的目的:
- R Script，对应文件后缀名.R;
- R markdown文件，对应文件后缀名是.Rmd。


---

### 包的安装和相关设置

- R和python之所以功能强大，很大原因在于这些语言背靠着开源社区，不断地有开发者发布和更新包，封装到程序包中供人使用。
    
- 在Global Options这个选项是指rstudio软件的全局设置，一般需要关心general、appearance、packages等。


---

### 界面介绍

左下角窗口：
    
- console是r语言指令的控制台，所有在左上角窗口中选中并运行的代码都可以在这里看到，也可以在下面直接输入R的代码；
- terminal可以向电脑的操作系统发送指令；
- RMarkdown则反映在rmd文件中生成新的文本文件时后台运行的情况。


---

### 界面介绍

右上角窗口:
    
- environment: value/vector/matrix/dataframe/list/function

```{r}
x<-c(1:10)
y<-log(x)
A<-data.frame(x,y)
f<-function(x){
  x+1
  }

```

- history
- connections
    
---

### 界面介绍

右下角窗口:
    
- plot
    
```{r,echo=FALSE, warning=FALSE, message=FALSE, out.width = "400px"}
rm(list = ls())#删除右上角所有对象

#在代码编辑器和control窗口输入下列代码，看看是否会产生如下结果
set.seed(1)
e<-rnorm(30)
x<-c(1:30)
y<-2+0.75*x+e
plot(x,y,type = 'p')
abline(lm(y~x),col='red')

```

---

### 界面介绍

右下角窗口:
    

- help

```{r,warning=FALSE,message=FALSE}
# help(lm)
# example(lm)

```


---


class: center, middle

## 2. R语言基础回顾


---

### R语言基础回顾


- R的数据结构
- 基于对象的编程
- R的环境
- R markdown



---


class: center, middle

## 3. 统计学习导论初步

---

### 统计学习导论教材

<center>
<img src="https://booksrun.com/image-loader/350/https:__m.media-amazon.com_images_I_41pP5+SAv-L.jpg" height="450" align="middle" />
</center>



.footnote[作者：Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani] 


---

第2章主要介绍统计学习的基本技术和概念，这章还包括了一类原理简单却在许多领域运用自如的**最近邻分类**方法。

第3章主要回顾**线性回归**，这是所有回归方法的基础。

第4章讨论了两类重要的分类模型:**逻辑斯谛回归**和**线性判别分析**。

第5章重点介绍**交叉验证**和**自助法**，这些方法可通过估计不同方法的精度选择最优的模型。

第6章提供了一类集经典与现代于一体的线性模型，这些方法是在标准线性回归基础上的改进，包括**逐步变量选择、岭回归、主成分回归、偏最小二乘和lasso 回归**。

第7章首先介绍一类在一元输入变量问题中颇有成效的非线性方法，之后将说明这些方法如何被运用到多于一个输入变量的非线性**可加**模型中。

第8章重点考察树类模型，包括**装袋法**、**提升法**和**随机森林**。

第9章中介绍**支持向量机**，它是一种既可以用于线性分类，也可以用于非线性分类的一组方法

第10章考虑只有输入变量没有输出变量的一类方法，重点讲述**主成分分析**、 $\mit{K}$**均值聚类**和**系统聚类**方法。


---

### 统计学习导论课程资源

- [斯坦福大学：统计学习](https://www.bilibili.com/video/BV11t411A7Ym?from=search&seid=12282984735614370936)
- [ISLR](https://github.com/rghan/ISLR) 
- [R for Statistical Learning](https://daviddalpiaz.github.io/r4sl/index.html)
- [An Introduction to Statistical Learning with R ](https://subasish.github.io/pages/ISLwithR/)


<center>
<img src="https://s3.ax1x.com/2021/03/07/6MCBh4.png" width="400" align="middle" />
</center>




---

### 统计学习的一些应用场景

- 确认前列腺癌的风险因子


```{r echo = F, fig.alig = "center"}
df.prostate = ElemStatLearn::prostate

pairs(df.prostate[, c("lpsa", "lcavol", "lweight", "age", "lbph", "svi", "lcp", "gleason", "pgg45")],
      col = "blueviolet", cex = .5, cex.axis = .5)
```


---

### 统计学习的一些应用场景

- 区分垃圾邮件

    - 建立一个垃圾邮件过滤器
    - 57个文本特征
    - 表格中挑选了一些能够将普通邮件和垃圾邮件区分开来的文本

```{r echo = F}
df.spam = mlr::spam.task$env$data
names(df.spam)[52] = "!" # change column name from "charExclamation" to "!"
spam = colMeans(df.spam[df.spam$type == "spam", -58])
email = colMeans(df.spam[df.spam$type == "nonspam", -58])
largest.diff = sort(abs(spam - email), decreasing = TRUE)[4:14]
knitr::kable(rbind(spam, email)[ ,names(largest.diff)], digits = 2)
```


---

### 统计学习的一些应用场景

- 手写体的识别

```{r Zip-data, echo=F, message=FALSE, warning=FALSE, include=FALSE}
# The following code is taken from the help file of the zip.train dataset
# from the R package ElemStatLearn
findRows <- function(zip, n) {
  # Find  n (random) rows with zip representing 0,1,2,...,9
  res = vector(length = 10, mode = "list")
  names(res) = 0:9
  ind = zip[,1]
  for (j in 0:9) {
    res[[j + 1]] = sample(which(ind == j), n) 
  }
  return(res)
}

digits = vector(length = 10, mode = "list")
names(digits) = 0:9
rows = findRows(zip.train, 6)
for (j in 0:9) {
    digits[[j+1]] = do.call("cbind", lapply(as.list(rows[[j+1]]), 
                            function(x) zip2image(zip.train, x)))
}
im = do.call("rbind", digits)
```

```{r echo=FALSE, fig.asp = 1}
image(im, col = gray(256:0/256), zlim = c(0,1), xaxt = 'n', yaxt = 'n')
```



---

### 统计学习的一些应用场景

- 收入和人口学变量之间的关系

```{r echo = FALSE, message= FALSE, warning=FALSE,fig.height=7, fig.width=10}

#Loading Wage dataset
Wage=ISLR::Wage
# dim(Wage)
# names(Wage)
Age=Wage$age
Wage_=Wage$wage

par(mfrow=c(1,2))
plot(Age,Wage_,col='gray',ylim=c(50,300),ylab='Wage')
smooth<-loess(Wage_ ~ Age, data=Wage) 
j <- order(Age)
lines(Age[j],smooth$fitted[j],col="red",lwd=3)
scatter.smooth(Wage_ ~ Age, span = 2/4, degree = 2,col='gray',lwd=3) #Another way

```


---

### 统计学习的一些应用场景

- 收入和人口学变量之间的关系

```{r echo = FALSE, message= FALSE, fig.height=7, fig.width=10}
par(mfrow=c(1,2))
Year=Wage$year
plot(Year,Wage_,col='gray',ylim=c(50,300),ylab='Wage')
abline(lm(Wage_ ~ Year,data=Wage),col='blue',lwd=3)
Education=Wage$education
boxplot(Wage_~Education,data=Wage, main="Boxplot of Education vs Wage",
        xlab="Education level", ylim=c(50,300),ylab="Wage",col=c(2,3,4,5,6))
```



---

### 统计学习的一些应用场景

- 股指变化的情况

    - 一个股票市场数据集（Smarket），该数据集收集了2001年至2005年间5年期的标准普尔500股票指数每日股指变动数据。
    - 这个数据的目标是用过去5天指数的变动比例预测5天后股指的涨跌状态。
    
    

```{r echo = FALSE, message= FALSE, fig.height=5, fig.width=10}
#Obtaining Figure 1.2
#Loading Smarket dataset
Smarket=ISLR::Smarket
# dim(Smarket)
# names(Smarket)
Pchange_Yesterday=Smarket$Lag1
Pchange_Twodays=Smarket$Lag2
Pchange_Threedays=Smarket$Lag3
Direction=Smarket$Direction
Year=Smarket$Year
par(mfrow=c(1,3))
boxplot(Pchange_Yesterday~Direction,data=Smarket, main="Yesterday. S&P500",
        xlab="Today?s direction", ylim=c(-4,6),ylab="Percentage change in S&P",col=c(4,2))
boxplot(Pchange_Twodays~Direction,data=Smarket, main="Two days ago. S&P500",
        xlab="Today?s direction", ylim=c(-4,6),ylab="Percentage change in S&P",col=c(4,2))
boxplot(Pchange_Threedays~Direction,data=Smarket, main="Three days ago. S&P500",
        xlab="Today?s direction", ylim=c(-4,6),ylab="Percentage change in S&P",col=c(4,2))

```





---

### 统计学习和机器学习

**cross-fertilization**

- 在有监督和无监督学习方面有重叠，而且区分很模糊

    - 机器学习追求大范围的应用和**预测的准确**
    - 统计学习更强调模型的**可解释性**，精确和不确定性
    - 在市场上，机器学习胜出
    


---

### trade off : 精度和可解释性

简单的模型可解释性强，复杂（光滑）的模型可解释性差。


<center>
<img src="https://s3.ax1x.com/2021/03/07/6MCs39.png" height="450" align="middle" />
</center>







---

### 有监督学习和无监督学习

有监督学习

- 回归、广义可加模型、提升法、支持向量机......

- 对结果Y进行测量
    
    - Y如果是连续的，就是一个**回归问题**
    - Y如果不连续，就是要给**分类问题**

- 对预测变量X进行了测量

- 有训练数据

    - 对测试数据进行预测
    - 理解哪些X影响了结果以及如何影响的
    - 评估预测和推论的质量
    
    

---

### 有监督学习和无监督学习

无监督学习


- 没有对结果Y进行测量

- 目标更模糊

    - 找到相似的case
    - 找到相似的特征
    - 找到变异最大的特征组合
    - ......

- 更难判断结果是否好



---

### 有监督学习和无监督学习

无监督学习

- 癌细胞的基因表达

    - 包含了64个癌症细胞系6830个基因表达测量数据
    - 在基因表达测量上的细胞系数据中看是否有集群存在，称这类问题为**聚类**(clustering)问题
    - 变量z1和z2是数据中的前两个**主成分**(principal component)，综合了各个细胞系6830个基因表达测量信息。



```{r echo = FALSE, message= FALSE, fig.height=4, fig.width=10}

par(mfrow=c(1,2))
Cols=function(vec){
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}


figure1_4_left2<-read.csv(file="COPY1_OF_figure1_4_islr.csv",header=T,sep=',',dec='.')

#Obtaining figure 1.4 left panel
plot(figure1_4_left2[,1:2],col=Cols(figure1_4_left2[,3]), pch=19, xlab="Z1", ylab="Z2",main='NCI60 in R2')

#Obtaining figure 1.4 right panel
nci.data=NCI60$data
nci.labs=NCI60$labs
# dim(nci.data)
pr.out=prcomp(nci.data, scale=TRUE)
# names(pr.out)
# dim(pr.out$x)
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19, xlab="Z1", ylab="Z2")

```

    

---

### 回归函数

$$Y=f(X)+\epsilon$$

- 线性：广告数据集

```{r echo = FALSE, message= FALSE, fig.height=5, fig.width=10}

Advertising<-read.csv(file="Advertising.csv",header=T,sep=',',dec='.')
# head(Advertising)
Sales=Advertising$sales
Tv=Advertising$TV
Radio=Advertising$radio
Newspaper=Advertising$newspaper
par(mfrow=c(1,3))
# plot(Tv,Sales,pch=19,col='red')
# plot(Radio,Sales,pch=19,col='red')
# plot(Radio,Sales,pch=19,col='red')
#dev.off()
#plot(Newspaper,Sales,pch=19,col='red')
t1 <- ggplot(Advertising, aes(Tv,Sales,color='red')) + 
  geom_point(color='red') +
  geom_smooth(method = "lm", se = FALSE, colour="purple")
t2 <- ggplot(Advertising, aes(Radio,Sales,color='red')) + 
  geom_point(color='red') +
  geom_smooth(method = "lm", se = FALSE, colour="purple")
t3 <- ggplot(Advertising, aes(Newspaper,Sales,color='red')) + 
  geom_point(color='red') +
  geom_smooth(method = "lm", se = FALSE, colour="purple")

gridExtra::grid.arrange(t1, t2, t3, nrow = 1, ncol = 3)

```


---

### 回归函数

$$Y=f(X)+\epsilon$$

- 非线性回归（LOESS，局部多项式回归）：收入数据集

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE}


Income=read.csv(file="Income1.csv",header=T,sep=',',dec='.')
YofEdu=Income$Education
Income_=Income$Income
#plot(YofEdu,Income_,pch=19,col='red',xlab='years of Education', ylab='Income')
tl <- ggplot(Income, aes(YofEdu,Income_,color='red')) + 
  geom_point(color='red') +
  labs(x='years of Education', y='Income')

# ggplot(Income, aes(YofEdu,Income_,color='red')) + 
  # geom_point(color='red') +
  # geom_smooth(method = "loess", se = FALSE, colour="purple") +
  # labs(x='years of Education', y='Income')

#With joining line segments

mod <- loess(Income ~ YofEdu, data = Income)
Income <- transform(Income, Fitted = fitted(mod))

tr <- ggplot(Income, aes(YofEdu,Income_,color='red')) + 
  geom_point(color='red') +
  geom_smooth(method = "loess", se = FALSE, colour="purple") +
  labs(x='years of Education', y='Income')+
  geom_segment(aes(x = YofEdu, y = Income,
                   xend = YofEdu, yend = Fitted))

#Joining line segments using base plot
# plot(Income ~ YofEdu, data = Income, type = "p", col = "red",
#      cex = 1.25,xlab='Years of Education')
# points(Fitted ~ YofEdu, data = Income)
# lines(Fitted ~ YofEdu, data = Income, col = "blue")
# with(Income, segments(YofEdu, Income, YofEdu, Fitted))
# 
# plot(Income ~ YofEdu, data = Income, type = "p", col = "red",pch=19,
#      cex = 1,xlab='Years of Education')
# points(Fitted ~ YofEdu, data = Income,pch=19,
#        cex = 0.5)
# lines(Fitted ~ YofEdu, data = Income, col = "blue")
# with(Income, segments(YofEdu, Income, YofEdu, Fitted))

gridExtra::grid.arrange(tl, tr, nrow = 1, ncol = 2)
```  




---

### 回归函数

- 多自变量非线性回归（LOESS，局部多项式回归）：收入数据集

```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}


Income3D=read.csv(file="Income2.csv",header=T,sep=',',dec='.')
# head(Income3D)
x=Income3D$Education
y=Income3D$Seniority
z=Income3D$Income
# dev.off()

xyz.fit <- loess(z~x+y+x*y,span=0.5,degree=2); d1<-0.5; d2<-10
f1<-function(x,y){predict(xyz.fit,cbind(x,y))};
x1<-sort(seq(min(x),max(x),d1)); y1<-sort(seq(min(y),max(y),d2)); z1<-outer(x1,y1,f1)
res<-persp(x=x1,y=y1, z=z1,phi=25,theta=40,shade=0.2,scale=TRUE,col="lightblue", expand=0.4,
           box = TRUE, xlab="years of Education",ylab="Seniority",zlab="Income")
mypoints <- trans3d(x, y, z, pmat=res); points(mypoints, pch=19, col="red")



```




---

### 回归函数：拟合效果检验



- 训练均方误差（training MSE）

$$MSE=\frac{1}{n}\sum_{i=1}^n (y_i-\hat{f}(x_i))^2$$


- 测试均方误差（test MSE）

对测试观测点 $(x_0,y_0)$来说，可计算：
$$Ave(y_0-\hat{f}(x_0))^2$$
- traning MSE很小的时候，test MSE可能会很大。



---

### 回归函数：拟合效果检验

经典回归分析：

$$Y=\hat{f}(X)+\epsilon$$
考虑预测的回归分析：


$$Y=f(X)+\epsilon$$

$$\hat{Y}=\hat{f}(X)$$
$$
\begin{aligned}
E(Y-\hat{Y})^2 &= E(f(X)+\epsilon-\hat{f}(X))^2\\
             &= \underbrace{[f(X)-\hat{f}(X)]}_{Reducible}+\underbrace{Var(\epsilon)}_{Irreducible}\\
\end{aligned}
$$




---

### 回归函数：拟合效果检验


存在多个training data的时候:

$$E(y_0-\hat{f}(x_0))^2 = Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))^2]+Var(\epsilon)$$

- 方差 $Var(\hat{f}(x_0))$：不同的数据集估计 $\hat{f}$ 的时候，函数的改变量。

    - 模型越光滑，不同数据集估计的函数的差异就越大。
    -  **模型越光滑，方差越大，反之越小** 。
    

- 偏差 $[Bias(\hat{f}(x_0))^2]$：真实的函数和构建的函数之间的差异。

    - 和我们经常构建的线性函数相比，任何一个真实的函数可能都很复杂。
    -  **模型越光滑，偏差越小，反之越大** 。


---

### 回归函数的估计

- 训练数据 $Tr$和测试数据 $Te$

    - 真实函数 $f(X)$和 $\hat{f}(X)$并不一致
    - 选择更光滑（felexible）的模型，但可能导致过拟合（overfit）
    
```{r echo = FALSE, message= FALSE, fig.height=5, fig.width=10}    



Income3D=read.csv(file="Income2.csv",header=T,sep=',',dec='.')
#head(Income3D)
x=Income3D$Education
y=Income3D$Seniority
z=Income3D$Income

#Fitting the plane

fit.lm<-lm(z~x+y)
#summary(fit.lm)
xnew <- seq(min(x), max(x), len=30)
ynew <- seq(min(y), max(y), len=30)
df <- expand.grid(x = xnew, y = ynew)
#dim(df)
f1<-function(x,y){predict(fit.lm,newdata=df)}
z2<-outer(xnew,ynew,f1)
persp(xnew,ynew,z2,phi=25,theta=40,shade=0.2,scale=TRUE,col="yellow", expand=0.4,
      box = TRUE, xlab="Years of Education",ylab="Seniority",zlab="Income")
mypoints <- trans3d(x, y, z, pmat=res); points(mypoints, pch=19, col="red")

```

---

### 回归函数的估计

- 训练数据 $Tr$和测试数据 $Te$

    - 不需要假设函数的形式
    - 无法将估计函数的问题简化为少量参数的估计，需要大量观测点
 
```{r echo=FALSE, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}


Income3D=read.csv(file="Income2.csv",header=T,sep=',',dec='.')
# head(Income3D)
x=Income3D$Education
y=Income3D$Seniority
z=Income3D$Income
#dev.off()

xyz.fit <- loess(z~x+y+x*y,span=0.5,degree=2); d1<-0.5; d2<-10
f1<-function(x,y){predict(xyz.fit,cbind(x,y))};
x1<-sort(seq(min(x),max(x),d1)); y1<-sort(seq(min(y),max(y),d2)); z1<-outer(x1,y1,f1)
res<-persp(x=x1,y=y1, z=z1,phi=25,theta=40,shade=0.2,scale=TRUE,col="lightblue", expand=0.4,
           box = TRUE, xlab="years of Education",ylab="Seniority",zlab="Income")
mypoints <- trans3d(x, y, z, pmat=res); points(mypoints, pch=19, col="red")

``` 




---

### trade off : 方差偏差权衡

模拟一个如下图的数据。

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE}

n = 100
target.fun = function(x) 5 * sin(2 * x) + 2 * cos(x + 1)
x = runif(n, 0, 6)
y = target.fun(x) + rnorm(n, 0, 1)
grid.x = data.frame(x = seq(0, 6, .01))
plot(x, y, pch = "+", col = rgb(0, 0, 0, alpha = .5))
lines(grid.x$x, target.fun(grid.x$x), col = "red", lwd = 2)

```  


---



### trade off : 方差偏差权衡

当阶数为7的时候，Te和Tr的MSE都比较小，达到模型复杂度的“sweet spot”。

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE, warning=FALSE}

B = 1000 # number of simulations
mse = function(y, y.hat) mean((y - y.hat)^2)
k = 12 # maximum degree of the polynomial we fit (model complexity)
ratio = .5 # train / test set split ratio
train.indices = c(rep(TRUE, ratio * n), rep(FALSE, (1 - ratio) * n))
train.errors = matrix(nrow = k, ncol = B)
test.errors = matrix(nrow = k, ncol = B)
for (b in (1:B)) {
  x = runif(n, 0, 6)
  y = target.fun(x) + rnorm(n, 0, 1)
  train.df = data.frame(x = x[train.indices], y = y[train.indices])
  test.df = data.frame(x = x[!train.indices], y = y[!train.indices])
  for (i in (1:k)) {
    fit = lm(y ~ poly(x, i, raw = TRUE), data = train.df)
    train.errors[i, b] = mse(train.df$y, predict(fit))
    test.errors[i, b] = mse(test.df$y, predict(fit, newdata = test.df))
  }
}
p.data = data.frame(k = rep(1:k, 2), error = c(rowMeans(train.errors), rowMeans(test.errors)), 
                    grp = c(rep("train", k), rep("test", k)))
ggplot(data = p.data, mapping = aes(x = factor(k), y = error, group = grp, color = grp)) +
  geom_point(size = 2) + geom_line(alpha = .2) + coord_cartesian(ylim = c(0, 30)) +
  xlab("Model Complexity") + ylab("Prediction Error") + 
  theme(legend.title = element_blank(), legend.position = c(.5, .7))

```  
    


---

### trade off : 方差偏差权衡

用不同阶数的多项式模型来拟合。

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE, warning=FALSE}
fit1 = lm(y ~ poly(x, 1, raw = TRUE), data = train.df)
fit7 = lm(y ~ poly(x, 7, raw = TRUE), data = train.df)
fit20 = lm(y ~ poly(x, 20, raw = TRUE), data = train.df)
p2.data = data.frame(x = grid.x$x, 
                     target.fun = target.fun(grid.x$x),
                     linear = predict(fit1, newdata = grid.x),
                     poly7 = predict(fit7, newdata = grid.x),
                     poly20 = predict(fit20, newdata = grid.x))
p2.data = reshape2::melt(p2.data, id = "x", variable.name = "group", value.name = "y")
ggplot(data = p2.data, mapping = aes(x = x, y = y, group = group, col = group)) +
  geom_line(size = 1, alpha = .8) + geom_point(data = train.df, mapping = aes(x = x, y = y), 
    inherit.aes = FALSE) + coord_cartesian(ylim = c(-7, 6))


```  




---

### 回归函数的估计


- 推论

    - 识别有效预测变量。
    - 响应变量和预测变量间的具体关系。
    - 响应变量和每个预测变量间关系是否线性？
    
- 例子
    
    - 影响中国人收入的因素？
    - 影响学生学业成就的因素？


    
    
---

### 分类问题

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE}

me = ElemStatLearn::mixture.example
df = data.frame(x1 = me$x[,1], x2 = me$x[,2], y = me$y)
knitr::kable(df[sample(200, 4),], row.names = FALSE)

```

容易得到回归系数

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE}
tsk = makeRegrTask(data = df, target = "y")
lrn.lm = makeLearner("regr.lm")
mod.lm = train(lrn.lm, tsk)
beta = coefficients(getLearnerModel(mod.lm))
knitr::kable(t(beta))
```  
    
    
    
---

### 分类问题


$$\hat{y}=b0+b_1x_1+b_2x_2$$

由于 $y>0.5$ 时为1，当 $y<0.5$ 时为0，所以分界线为：

$$b0+b_1x_1+b_2x_2 = 0.5$$

那么：

$$x_2 = \frac{0.5-b_0-b_1x_1}{b3}$$

   
---

### 分类问题

线性分割的结果

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE}
grid = expand.grid(x1 = seq(-2.6, 4.2, .1), x2 = seq(-2.0, 2.9, .05))
y.hat = getPredictionResponse(predict(mod.lm, newdata = grid))
grid["y.lm"] = factor(as.numeric(y.hat > .5))

db = function(x1, coef = beta) {
  (coef[3])^-1 * (0.5 - coef[1] - coef[2] * x1)
}

ggplot() + 
  geom_point(aes(x = grid$x1, y = grid$x2, col = grid$y.lm), shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
  geom_line(aes(x = grid$x1, y = db(grid$x1))) +
  geom_point(aes(x = df$x1, y = df$x2, col = factor(df$y)), shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
  scale_colour_manual(values = c("deepskyblue", "orange")) +
  theme_void()
```  
    
    
 

---

### 分类模型：分类效果检验



- 训练错误（training error）

$$\frac{1}{n}\sum_{i=1}^n I(y_i \neq \hat{y_i}))^2$$


- 测试错误（test error）

对测试观测点 $(x_0,y_0)$来说，可计算：
$$Ave(I(y_0 \neq \hat{y_0}))^2$$ 
 
    
    
---

### 分类问题

kNN的方法之15NN的结果

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE}

tsk2 = makeClassifTask(data = data.frame(df[,1:2], y = factor(df$y)), target = "y")
lrn.knn15 = makeLearner("classif.knn", k = 15)
mod.knn15 = train(lrn.knn15, tsk2)
Plot2DBinaryClassification = function(task, model, grid.res = 100) {
  data = getTaskData(task, target.extra = TRUE)
  x1 = data$data[, 1]
  x2 = data$data[, 2]
  grid = expand.grid(x1 = seq(min(x1), max(x1), length.out = grid.res), 
    x2 = seq(min(x2), max(x2), length.out = grid.res))
  y.hat = getPredictionResponse(predict(model, newdata = grid))
  ggplot() + 
    geom_point(aes(x = grid$x1, y = grid$x2, col = y.hat), shape = 20, size = .05, alpha = .5,show.legend = FALSE) +
    geom_contour(aes(grid$x1, grid$x2, z = as.numeric(y.hat)), col = "black", bins = 1) +
    geom_point(aes(x = x1, y = x2, col = data$target), shape = "o", size = 4, stroke = 2, show.legend = FALSE) +  
    scale_colour_manual(values = c("deepskyblue", "orange")) +
    xlab(names(data$data)[1]) + 
    ylab(names(data$data)[2])
}
Plot2DBinaryClassification(task = tsk2, model = mod.knn15) + theme_void()
```  
    
    
    
    
---

### 分类问题

1NN的结果：

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE}

lrn.knn1 = setHyperPars(lrn.knn15, k = 1)
mod.knn1 = train(lrn.knn1, tsk2)
Plot2DBinaryClassification(task = tsk2, model = mod.knn1)

```  
    
    
    

 

    

    
---

### trade off : 方差偏差权衡

在KNN分类中对K的选择

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE}

me = ElemStatLearn::mixture.example
df = data.frame(x1 = me$x[,1], x2 = me$x[,2], y = me$y)
means.indices = c(sample(1:10, 5000, replace = TRUE), sample(11:20, 5000, replace = TRUE))
X = me$means[means.indices, ] + mvtnorm::rmvnorm(10000, rep(0, 2), .2 * diag(2))

df.test = data.frame(x1 = X[ , 1], x2 = X[ , 2], y = c(rep(0, 5000), rep(1, 5000)))
df.full = rbind(df, df.test)
df.full$y = factor(df.full$y)
tsk = makeRegrTask(data = df, target = "y")
lrn.lm = makeLearner("regr.lm")
mod.lm = train(lrn.lm, tsk)
set.seed(1986)



#knitr::kable(df.test[c(1, 10000), ]) # show first and last simulated point
y.hat = as.numeric(getPredictionResponse(predict(mod.lm, newdata = df.full)) > .5)
train.error.lm = 1 - mean(y.hat[1:200] ==  df$y)
test.error.lm = 1 - mean(y.hat[201:10200] ==  df.test$y)
```  
    

```{r echo=FALSE, fig.height=6.5, fig.width=10, message=FALSE}

rin = makeFixedHoldoutInstance(train.inds = 1:200, test.inds = 201:10200, size = 10200) # defining a resampling instance
rin$desc$predict = "both"

tsk3 = makeClassifTask(data = df.full, target = "y")

k = c(1, 3, 5, 7, 9, 11, 15, 17, 23, 25, 35, 45, 55, 83, 101, 151)
ps = makeParamSet(makeDiscreteParam("k", k))
mmceTrainMean = setAggregation(mmce, train.mean)
models = tuneParams("classif.knn", tsk3, resampling = rin, par.set = ps,
                    measures = list(mmce, mmceTrainMean), control = makeTuneControlGrid())
results = generateHyperParsEffectData(models)
plot.data = data.frame(k, train = results$data$mmce.train.mean, test = results$data$mmce.test.mean)
plot.data = reshape2::melt(plot.data, id = "k")
ggplot(data = plot.data, aes(x = k, y = value, group = variable, col = variable)) +
  geom_line() + geom_point() +
  # linear model train and test error:
  geom_point(aes(x = 200/3, y = train.error.lm, col = "train"), shape = 15, size = 3) +
  geom_point(aes(x = 200/3, y = test.error.lm, col = "test"), shape = 15, size = 3) +
  # inversion and log trafo of x axis:
  scale_x_continuous(trans = scales::trans_new('nlog2_trans',
                                              transform = function(x) {-log2(x)},
                                              inverse = function(x) {2**(-x)},
                                              breaks = scales::log_breaks(20, 2))) +
  # visual tweaks:
  scale_color_manual(name = element_blank(), values = c("deepskyblue", "orange")) +
  ylab("Missclassification error") + theme(legend.position="bottom")

```  
    
    
    
---

class: center, middle

# 谢谢

本幻灯片由 R 包 [**xaringan**](https://github.com/yihui/xaringan) 生成；
    
    
 
    
    
    
    
